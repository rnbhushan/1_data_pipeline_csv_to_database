import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import pickle
import torch

# Step 1: Load Data from Excel
data = pd.read_excel("access_requests.xlsx")  # Replace with your file path

# Step 2: Preprocess Data
data.dropna(inplace=True)
data['Decision'] = data['Decision'].apply(lambda x: 1 if x == 'Granted' else 0)

# Features and Target
X = data['Comments'] + " " + data['Website']
y = data['Decision']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Text Vectorization (TF-IDF)
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train the Classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train_tfidf, y_train)

# Save the Model and Vectorizer
with open("decision_model.pkl", "wb") as model_file:
    pickle.dump(model, model_file)
with open("tfidf_vectorizer.pkl", "wb") as vectorizer_file:
    pickle.dump(vectorizer, vectorizer_file)

# Evaluate Model Accuracy
y_pred_test = model.predict(X_test_tfidf)
print("Test Accuracy:", accuracy_score(y_test, y_pred_test))

# Step 4: Load GPT Model for Context Generation
model_name = "gpt2"  # Replace with a suitable GPT model
tokenizer = AutoTokenizer.from_pretrained(model_name)
gpt_model = AutoModelForCausalLM.from_pretrained(model_name)

# Enable GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
text_generator = pipeline(
    "text-generation",
    model=gpt_model,
    tokenizer=tokenizer,
    truncation=True,
    device=0 if device == "cuda" else -1
)
print(f"Using device: {device}")

# Step 5: Predict Decision and Generate Context
def predict_access_with_context(comment, website):
    # Load the saved model and vectorizer
    with open("decision_model.pkl", "rb") as model_file:
        clf = pickle.load(model_file)
    with open("tfidf_vectorizer.pkl", "rb") as vectorizer_file:
        vectorizer = pickle.load(vectorizer_file)
    
    # Step 1: Predict Decision
    input_text = comment + " " + website
    input_vector = vectorizer.transform([input_text])
    decision = clf.predict(input_vector)[0]
    decision_text = "Granted" if decision == 1 else "Rejected"

    # Step 2: Generate Context
    prompt = (
        f"Based on the following comment and website, explain why the decision was '{decision_text}'.\n"
        f"Comment: {comment}\nWebsite: {website}\nExplanation:"
    )
    response = text_generator(
        prompt,
        max_length=100,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id
    )
    context = response[0]['generated_text']

    return {"Decision": decision_text, "Context": context}

# Step 6: Test with New Data
test_comment = "User is not authorized to access customer data directly."
test_website = "devops.example.com"

result = predict_access_with_context(test_comment, test_website)
print("Decision:", result["Decision"])
print("Context:", result["Context"])


=====================

3. Preload and Cache the Model
If you have internet access, download and cache the model locally:

Download the Model: Run this script to load and cache the gpt2 model and tokenizer:

python
Copy code
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "gpt2"
AutoModelForCausalLM.from_pretrained(model_name)
AutoTokenizer.from_pretrained(model_name)
print("Model and tokenizer successfully downloaded and cached!")
Verify the Cache: By default, Hugging Face stores models in ~/.cache/huggingface/transformers/. Check this directory to ensure the gpt2 model and its files are present (e.g., config.json, pytorch_model.bin, tokenizer.json).




4. Offline Mode Setup
If the system doesn't have internet access:

Download the Model Files Manually:

Visit the GPT-2 Model Page.
Download the following files:
config.json
pytorch_model.bin
vocab.json
merges.txt
tokenizer_config.json
Save the Files Locally: Place these files in a directory, e.g., /path/to/local/gpt2.

Load Model from Local Directory: Update your code to use the local directory instead of fetching from Hugging Face's servers:

python
Copy code
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "/path/to/local/gpt2"
model = AutoModelForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)
print("Loaded GPT-2 model and tokenizer from local directory!")
5. Test with OpenAI GPT-3 or GPT-4 Instead
If you're using OpenAI's GPT-3 or GPT-4, the model is accessed via API and doesn't require Hugging Face's libraries or downloads. Make sure:

You have an active internet connection.
Your OpenAI API key is configured correctly.
Troubleshooting
Check Permissions: Ensure you have the necessary permissions to read/write to the cache directory (~/.cache/huggingface/transformers).

Clear Corrupted Cache: If files are partially downloaded or corrupted, clear the cache:

bash
Copy code
rm -rf ~/.cache/huggingface/transformers
Proxy Configuration: If you're behind a corporate firewall or proxy, configure proxy settings for Python.

Alternative Solution
If the above doesn't resolve the issue or you need immediate functionality, switch to OpenAI GPT (e.g., GPT-3 or GPT-4) as it only requires API access and doesn't rely on local caching.

Let me know if you need further assistance!

